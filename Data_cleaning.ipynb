{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f938a4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**URL filtering (llama 3.1)**  \n",
    "\"After escaping the URL blacklist (adult/website links) and bypassing the URL classification and filtering system, in order to allow everyone to work more efficiently, I removed the internal restrictions on URLs from sources like Arxiv and Wikipedia.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01931da7",
   "metadata": {},
   "source": [
    "**Content extraction**  \n",
    "\"After escaping the URL blacklist, I extracted the text content in the URL [text content], needed to filter the source [source], [content] I initially thought was internal, used trafilatura to extract internal content, trafilatura's usage method is relatively simple:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5db78",
   "metadata": {},
   "source": [
    "**Speech Recognition**  \n",
    "1. \"Used FastText to train a language recognition model, supporting language recognition for texts with a length of 0.65.\"  \n",
    "2. \"Used langid for language recognition.\"  \n",
    "3. \"Used data_juicer/ops/filter/language_id_score_filter.py for language recognition.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351f134",
   "metadata": {},
   "source": [
    "The text appears to be a technical explanation in Chinese about fine-tuning a language model (specifically LLaMA). I'll translate and break it down into English, then explain it in a simplified way to help you understand the concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Translation into English\n",
    "\n",
    "#### Quality Verification\n",
    "- **Data Quality Verification**: First, remove some data with obvious quality issues and duplicate content, such as spam (e.g., promotional ads); then, if necessary, further filter data for length, whether it contains long text, and remove long text with machine-generated characteristics like repetitive text.\n",
    "- **Summoning Series Machine Verification**: Some of the initially used children (subsets of data) can be filtered using rules or not, depending on the needs. For example, if the data is academic or involves multilingual recognition, you can use (the academic rules are mainly used in practice):\n",
    "  a. Summon the main academic university dictionary (reference).\n",
    "  b. Use user-generated text dictionary (good quality).\n",
    "  c. Simply convert the text into a model, such as tokenization, stemming, stop words (reference).\n",
    "  d. If the word count is less than 10, remove it from the middle and bottom: * word (reference, usage), start, * word (usage, more references), end.\n",
    "- **Using Mainstream Pretrained Models for Verification**: The pretrained models used here mainly rely on pretrained models with good quality for filtering.\n",
    "  **Specific Steps**:\n",
    "  a. Use LLaMA to determine the natural language vocabulary, which can filter data but does not have its own text.\n",
    "  b. Data length and type are not just one type, such as http links, multiple machines can filter them individually, the text compares English with other texts, such as \"language\" and \"research\" data, which is English-language academic data, and the language is named and supported after mainstream pretrained models are used. After using the pretrained model for filtering, it is determined to be necessary.\n",
    "  c. Use the perplexity academic dictionary used in the academic field, where the dictionary is not specific to children.\n",
    "  d. The machine converts according to the machine model, producing a model that can be used for inference.\n",
    "- **Summoned Verification, the Actual Machine is Only a Very Precise Machine**: Some of the text in the world has control; after verification using pretrained models, the machine will be marked and used for inference and filtering.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This text is about preparing and cleaning data to fine-tune a language model like LLaMA (a type of AI model used for understanding and generating text). Fine-tuning means improving the model by training it on specific, high-quality data. Here's a breakdown of what they're doing:\n",
    "\n",
    "#### 1. **Data Quality Verification (Cleaning the Data)**\n",
    "- **What They're Doing**: They start by removing bad or useless data, like spam (e.g., ads) or duplicate content.\n",
    "- **Why**: Bad data can confuse the model and make it perform poorly.\n",
    "- **How**:\n",
    "  - They filter out text thatâ€™s too long or repetitive (e.g., machine-generated text that keeps repeating the same thing).\n",
    "  - They check the length of the text and remove anything that doesnâ€™t fit their needs.\n",
    "\n",
    "#### 2. **Summoning Series Machine Verification (Filtering with Rules)**\n",
    "- **What They're Doing**: They apply specific rules to filter the data further, especially if the data is for academic use or involves multiple languages.\n",
    "- **Why**: They want to make sure the data is relevant and useful for the model.\n",
    "- **How**:\n",
    "  a. They use a dictionary of academic words to filter data (e.g., keeping only academic-related text).\n",
    "  b. They use a dictionary of user-generated text thatâ€™s known to be good quality.\n",
    "  c. They process the text by breaking it into smaller pieces (tokenization), simplifying words (stemming), and removing unimportant words (stop words like \"the\" or \"and\").\n",
    "  d. They remove very short texts (less than 10 words) because they might not be useful.\n",
    "\n",
    "#### 3. **Using Pretrained Models for Verification (Advanced Filtering)**\n",
    "- **What They're Doing**: They use a pretrained model (like LLaMA) to check the quality of the data.\n",
    "- **Why**: Pretrained models can better understand language and help filter out bad data.\n",
    "- **How**:\n",
    "  a. They use LLaMA to identify natural language patterns and filter out data that doesnâ€™t fit.\n",
    "  b. They check the type of data (e.g., removing links like \"http\") and compare languages (e.g., keeping only English academic data).\n",
    "  c. They use a concept called \"perplexity\" (a measure of how predictable the text is) to filter academic data. Lower perplexity means the text is more predictable and likely better quality.\n",
    "  d. They convert the text into a format the model can understand and use it for further filtering.\n",
    "\n",
    "#### 4. **Final Verification**\n",
    "- **What They're Doing**: After all the filtering, they use the cleaned data to fine-tune the model.\n",
    "- **Why**: They want to make sure only the best data is used to improve the model.\n",
    "- **How**: They double-check the data with the pretrained model to ensure itâ€™s high quality before using it.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts to Understand\n",
    "1. **Data Cleaning**: Removing bad or irrelevant data (like spam or duplicates) to make sure the model learns from good data.\n",
    "2. **Filtering with Rules**: Using specific rules (like word length or academic dictionaries) to keep only the data you want.\n",
    "3. **Pretrained Models**: Models like LLaMA that are already trained on a lot of data and can help filter or process new data.\n",
    "4. **Perplexity**: A way to measure how \"confusing\" a piece of text is to the model. Lower perplexity means the text is easier for the model to understand.\n",
    "5. **Fine-Tuning**: Training a model on specific data to make it better at a particular task (e.g., understanding academic texts).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a056123",
   "metadata": {},
   "source": [
    "Sure! Here's a translation and explanation of the content in the image:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Model Scoring (æ¨¡åž‹æ‰“åˆ†)\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Using models to score the **quality of pretraining data** has become a standard method for data cleaning. Technologies behind models like **llama3** and **qwen2** have all mentioned this. In this context:\n",
    "\n",
    "* **BERT-based models** are especially suitable because they are **encoder-based (transformer) models** that are good at representing text.\n",
    "* These BERT-type models can be used to **score the quality of data**, or we can even use **stronger instruction-tuned models** like GPT-4o to do it.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "The idea is to train a classifier that distinguishes between **high-quality** and **low-quality** training data. Hereâ€™s how:\n",
    "\n",
    "* You **manually select a set of high-quality samples** and **low-quality samples** as labeled data.\n",
    "* These labeled samples are used to train a model (like BERT) to **predict the quality** of other unlabelled data.\n",
    "\n",
    "Training a scoring model is not about reaching 100% accuracy â€” the goal is **practical separation**. Even if the classifier is not perfect, as long as it separates high and low-quality data reasonably well, itâ€™s useful.\n",
    "\n",
    "Some notes:\n",
    "\n",
    "* Even if your scoring model isnâ€™t super strong, itâ€™s better to have one than none.\n",
    "* For example, if you spend a month training a scoring model, it could be more effective than manually cleaning 32K samples.\n",
    "* Suppose you have 32K total samples â€” 4K low quality and 28K high quality. You can use the model to filter or re-rank the data.\n",
    "\n",
    "### Recommended tools:\n",
    "\n",
    "* **Bert-base** or **FastText** are commonly used for this task.\n",
    "* You just need to fine-tune them slightly.\n",
    "* You can refer to Deepseek-Mathâ€™s [8.1 Technical Report](https://example.com) (placeholder for the link) for more details.\n",
    "\n",
    "---\n",
    "\n",
    "## â›“ Flowchart Explanation (Bottom Image):\n",
    "\n",
    "This shows the **pipeline to collect and score math-related data** from the Common Crawl:\n",
    "\n",
    "1. **Train a FastText Model**\n",
    "\n",
    "   * Use seed data (math-related samples) to train a FastText classifier.\n",
    "\n",
    "2. **Recall Math-Related Webpages From Common Crawl**\n",
    "\n",
    "   * Use the trained model to find math-related webpages from a massive dataset (Common Crawl: 40B HTML pages).\n",
    "\n",
    "3. **Discover Math-Related Domains**\n",
    "\n",
    "   * From those pages, identify domains that are especially math-heavy.\n",
    "\n",
    "4. **Annotate Math-Related URL Path From Labelers**\n",
    "\n",
    "   * Human labelers review and annotate the relevant URLs to confirm quality.\n",
    "\n",
    "All of this helps build a high-quality **Math Corpus** for training models.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if youâ€™d like help replicating this process in your own project!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168d0fd",
   "metadata": {},
   "source": [
    "Certainly! Here's an English translation and explanation of the content in the second image:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Data Deduplication (æ•°æ®åŽ»é‡)\n",
    "\n",
    "In the **pretraining process**, data deduplication is critical and is generally categorized into three main types:\n",
    "\n",
    "### Three Types of Deduplication:\n",
    "\n",
    "1. **Within the Training Dataset**\n",
    "\n",
    "   * This involves deduplication across **multiple types** and **sources** within the dataset.\n",
    "   * It includes:\n",
    "\n",
    "     * **Within-document deduplication** (like removing repeated lines, paragraphs, n-grams).\n",
    "     * **Cross-document deduplication** (e.g., same content appearing in different files).\n",
    "     * **Model-based deduplication** where similar outputs across different model checkpoints are filtered (e.g., LLaMA filtered both raw CommonCrawl and C4 data to avoid overlap).\n",
    "\n",
    "2. **Deduplication Based on Training Frequency**\n",
    "\n",
    "   * If a sample appears multiple times during training (multiple **epochs**), deduplication should be handled depending on data type and source.\n",
    "   * Some datasets are trained with different epochs, and deduplication strategies should account for that.\n",
    "\n",
    "3. **Deduplication for Evaluation Datasets**\n",
    "\n",
    "   * Ensure **no overlap** between training data and evaluation data (e.g., for measuring language modeling loss or performance on benchmarks).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Deduplication Workflow (Process Overview):\n",
    "\n",
    "| Step       | Focus                                  | Description                                                                                                                                                                                                                                                                              |\n",
    "| ---------- | -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Step 1** | Define the \"Unit\"                      | Based on the dataâ€™s source and structure, determine what the basic deduplication unit is (e.g., file, line, paragraph). Examples: LLaMA uses line-level deduplication for CommonCrawl; Book3 uses book-level deduplication (if >90% overlap); GitHub code uses file-level deduplication. |\n",
    "| **Step 2** | Deduplication **within the Unit**      | Remove duplicate lines, paragraphs, or n-grams **within** a single Unit. For example, detect and remove repeated lines in one document. This step can be seen as a form of \"low-quality\" filtering (e.g., Gopher).                                                                       |\n",
    "| **Step 3** | Deduplication **between Units**        | Compare different Units to remove near-duplicate or exact duplicate content across different files/documents. Keep only unique Units (e.g., Gopher).                                                                                                                                     |\n",
    "| **Step 4** | Deduplicate test set from training set | Filter out any test data that overlaps with the training data to avoid data leakage during evaluation.                                                                                                                                                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Common Techniques\n",
    "\n",
    "Techniques like **SimHash**, **MinHash** are used to deduplicate text at scale. These help estimate **how many unique training samples** are needed.\n",
    "\n",
    "ðŸ“ **Guideline**:\n",
    "You typically need about **10 trillion tokens** of data to train a model, and with **80% deduplication precision**, the results are generally reliable.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like to implement or test any of these deduplication methods in your pipeline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613195c",
   "metadata": {},
   "source": [
    "Absolutely! Here's the English translation and explanation of the third image:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¬ Exploring Data Diversity (æ•°æ®å¤šæ ·æ€§æŽ¢ç´¢)\n",
    "\n",
    "Data diversity has many dimensions:\n",
    "\n",
    "* **Task diversity**\n",
    "* **Linguistic diversity**\n",
    "* **Domain/topic diversity**\n",
    "* **Data source diversity**\n",
    "\n",
    "In large-scale model training, **data quality and diversity** are critical. While different companies use data from various sources, **only about 0.5%** of data may be high quality. So, it's essential to carefully select and **filter for diversity**.\n",
    "\n",
    "### ðŸ‘‰ A Common Strategy:\n",
    "\n",
    "**Generalization â†’ Clustering â†’ Core Sample Selection (ä¸‰æ­¥æ³•)**\n",
    "\n",
    "ðŸ“˜ **Reference:**\n",
    "**LIMA**: *Less Is More for Alignment; maybe only 0.5% data is needed*\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Illustration (Right Diagram Summary)\n",
    "\n",
    "The image shows the process of selecting diverse and high-quality samples:\n",
    "\n",
    "1. **Encoding**: Sentences are converted into embeddings.\n",
    "2. **Mean Pooling + Normalization**: Data is preprocessed into a unified format.\n",
    "3. **Clustering**: Samples are grouped in latent space.\n",
    "4. **Sampling**: Samples are drawn from clusters to ensure diversity.\n",
    "5. **LLM Fine-Tuning & Evaluation**: The selected samples are used to tune LLMs and evaluate them using various tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## â­ Techniques to Enhance Data Diversity\n",
    "\n",
    "Use **SimBERT** for generating embeddings. Quality scores can come from a **dedicated quality scoring model**.\n",
    "\n",
    "### **Three Strategies for Selecting Diverse Samples:**\n",
    "\n",
    "#### **Option 1: K-means clustering-based diversity sampling**\n",
    "\n",
    "* Cluster data using k-means and pick representative samples.\n",
    "* **Limitation**: This may prefer poor-quality samples if they dominate the clusters.\n",
    "\n",
    "#### **Option 2: Class-weighted sampling + diversity scoring**\n",
    "\n",
    "* Use both class-based diversity scores and quality scores to select data.\n",
    "* **Limitation**: Poor clustering can lead to semantically similar samples being scattered across clusters.\n",
    "\n",
    "#### **Option 3 (Recommended): KNN clustering + similarity-based diversity sampling**\n",
    "\n",
    "* Use KNN to group similar samples, compute **average similarity per class**, and **reweight based on quality**.\n",
    "* Finally, **resample based on both diversity and quality**.\n",
    "* **Result**: The final data distribution closely approximates a normal distribution.\n",
    "\n",
    "ðŸ“˜ **Reference:**\n",
    "**D4**: *Improving LLM Pretraining via Document De-Duplication and Diversification*\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Tip:**\n",
    "Semantically diverse sentences with the same meaning can still convey different knowledge â€” managing this helps maintain useful variation in training.\n",
    "\n",
    "Let me know if you want code examples or tools to implement these techniques!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6e49c",
   "metadata": {},
   "source": [
    "Sure! Here's a full English explanation of the content in the image:\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Vertical Domain Experience**\n",
    "\n",
    "### **Overview Diagram (Top Section)**\n",
    "\n",
    "This flowchart describes the process of building a domain-specific (vertical) dataset from general data:\n",
    "\n",
    "1. **Keyword Mining**:\n",
    "\n",
    "   * Extract keywords from initial seed data (basic data you start with).\n",
    "2. **Semantic Similarity-Based Recall for Candidate Seed Corpus**:\n",
    "\n",
    "   * Use these keywords to retrieve related documents from a larger, general-purpose corpus based on semantic similarity.\n",
    "   * Filter by humans to create a cleaned seed corpus.\n",
    "3. **Train Vertical Domain Classifier**:\n",
    "\n",
    "   * Use this seed corpus to train a classifier that distinguishes between relevant and irrelevant data in the domain.\n",
    "   * Use the classifier to filter the current large dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps (Bottom Section)**\n",
    "\n",
    "The method mainly references how **DeepseekMath** handles vertical domain data:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Crawl as Much General Data as Possible**\n",
    "\n",
    "* Used as the starting raw material (initial data library).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. From the Chinese corpus, extract domain-related data through clustering and classification.**\n",
    "\n",
    "* The goal: Identify domain-specific data, like documents from open-source encyclopedias, technical books, or web articles that are related to the domain.\n",
    "* Expand the seed data through a structured pipeline:\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Overview**\n",
    "\n",
    "**Keyword Initialization from General Data â†’ Retrieve Top 10 Most Similar Documents to Seed Data â†’ Manual Filtering**\n",
    "\n",
    "---\n",
    "\n",
    "### **a. Keyword Initialization from General Data**\n",
    "\n",
    "i. **Preprocessing**:\n",
    "\n",
    "* Clean the seed data and split text into smaller segments to retain useful phrase-level information.\n",
    "* This helps classify documents with different topics or sections.\n",
    "\n",
    "ii. **Keyword Extraction**:\n",
    "\n",
    "* Use Jieba (a Chinese word segmentation tool) to extract keywords from the segments.\n",
    "* Reference tool: \\[138 Chinese NLP Tools Handbook - New Word Discovery].\n",
    "\n",
    "iii. **Keyword Filtering**:\n",
    "\n",
    "* Use TF-IDF and frequency analysis to do a first round of filtering, followed by human review to finalize the keyword list.\n",
    "\n",
    "iv. **Search Top-k Documents Using Keywords**:\n",
    "\n",
    "* Use the keywords to retrieve the most related documents (e.g., top 10 per keyword).\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Retrieve Top 10 Documents Most Similar to Seed Data**\n",
    "\n",
    "i. **Embedding-Based Similarity**:\n",
    "\n",
    "* Convert documents into embeddings using 512-token segments, and calculate semantic similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### **c. Human Filtering**\n",
    "\n",
    "i. **Final Filtering**:\n",
    "\n",
    "* Manually review the retrieved data and select the most relevant documents as the final seed corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Train a Classifier to Identify Domain-Specific Data**\n",
    "\n",
    "a. **Training Data**:\n",
    "\n",
    "* Positive examples: Clean seed documents (manually verified).\n",
    "* Negative examples: Non-relevant documents selected via random sampling (same document length).\n",
    "\n",
    "b. **Model Selection**:\n",
    "\n",
    "* Use a 1024-token BERT model.\n",
    "\n",
    "c. **Model Application**:\n",
    "\n",
    "* Use the trained model to classify large-scale Chinese datasets.\n",
    "* For each document, extract the top 1k tokens and run classification.\n",
    "* Choose documents with >0.5 confidence as domain-relevant.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a visual version of this or if you'd like it adapted to a specific use case or domain (like medical, legal, etc.)!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
