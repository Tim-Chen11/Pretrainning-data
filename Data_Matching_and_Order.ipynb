{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5912998",
   "metadata": {},
   "source": [
    "Absolutely! Here‚Äôs a full English explanation of the second image you uploaded:\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2.4 Data Matching and Order**\n",
    "\n",
    "---\n",
    "\n",
    "### **üß≠ Guiding Principles**\n",
    "\n",
    "To get better results, it‚Äôs ideal to organize tasks by categories. For example, divide the data into **Chinese**, **English**, and **Code**, then train separate **classifiers** for each.\n",
    "\n",
    "* Each **document** should be judged as a whole rather than sentence-by-sentence.\n",
    "* Split into general categories like encyclopedia, code, markdown, etc.\n",
    "* Then use classifiers (e.g., BERT-based) to automatically manage this split.\n",
    "* Data volume per class should be around **20 categories**.\n",
    "\n",
    "Most technical documents are a mix of knowledge + code. So:\n",
    "\n",
    "* Split into **three types**:\n",
    "\n",
    "  1. **Knowledge**\n",
    "  2. **Code**\n",
    "  3. **Reasoning (e.g., step-by-step math)**\n",
    "\n",
    "This is a **logical classification** for Chinese knowledge content.\n",
    "\n",
    "### **Recommended Data Proportions for Chinese Models**:\n",
    "\n",
    "**Chinese : English : Code = 4 : 4 : 2**\n",
    "\n",
    "* Chinese should be more than 50%\n",
    "* English must be high quality, but not overly dominant\n",
    "* If the model needs strong math/reasoning abilities, **code data** is especially important\n",
    "\n",
    "---\n",
    "\n",
    "### **üè≠ Industrial Best Practices**\n",
    "\n",
    "* For special domain training or fine-tuning (like math, QA, summarization), use **continue pretraining** on a **mixture of general and domain data**.\n",
    "\n",
    "### **Key Rule:**\n",
    "\n",
    "> **Domain data should not exceed 15%**\n",
    "\n",
    "* More than that may harm general capability\n",
    "* When mixing general and domain data, make sure to blend well\n",
    "* Suggested domain-to-general ratio: **10%‚Äì15%**\n",
    "* For pretraining/fine-tuning, **empirical testing is needed**\n",
    "\n",
    "### **Scaling Law Practice:**\n",
    "\n",
    "* Use **small models** to experiment with ratios first\n",
    "* Apply scaling law to infer performance on larger models\n",
    "* For example:\n",
    "  If a **1:1 ratio** of domain\\:general performs poorly in small model, it likely won‚Äôt scale well\n",
    "\n",
    "---\n",
    "\n",
    "### **üìö Curriculum Learning**\n",
    "\n",
    "Large models usually learn with a **curriculum approach**:\n",
    "\n",
    "> **First learn easy tasks ‚Üí then hard ones**\n",
    "\n",
    "The **order of data input** matters.\n",
    "Data must be **fed in stages** during training to help models acquire capabilities progressively.\n",
    "\n",
    "Three examples of curriculum learning for capability training:\n",
    "\n",
    "---\n",
    "\n",
    "### **üë®‚Äçüíª Code Capability**\n",
    "\n",
    "* To train **code generation**, LLaMA-2 was fine-tuned into **CodeLLaMA**\n",
    "* **Curriculum design**:\n",
    "\n",
    "  1. 2T general token pretraining\n",
    "  2. Then 500B code token pretraining\n",
    "* **CodeLLaMA-Python** used:\n",
    "\n",
    "  * 2T general token\n",
    "  * 500B code-related token\n",
    "  * 100B Python-specific code token\n",
    "\n",
    "---\n",
    "\n",
    "### **üìê Math/Numerical Reasoning**\n",
    "\n",
    "* **CodeLLaMA** was used for math tasks\n",
    "* Training method:\n",
    "\n",
    "  * Start with general token + 50‚Äì200B math token\n",
    "  * Add math-related code (LaTeX, formulas, solutions)\n",
    "* Found that:\n",
    "\n",
    "  * Adding math data (even if just 5%) **preserves general ability**\n",
    "  * Mixing code helps models understand math better\n",
    "\n",
    "---\n",
    "\n",
    "### **üìú Long Context Ability**\n",
    "\n",
    "* Many models use **continued pretraining** on long texts to gain long-context understanding\n",
    "* Use **RoPE extrapolation** (positional embedding method)\n",
    "* **Example**:\n",
    "\n",
    "  * **CodeLLaMA** extended context window from 4k ‚Üí 100k\n",
    "  * Training curriculum:\n",
    "\n",
    "    * Start small (2.5T tokens with ‚â§16k context)\n",
    "    * Move to larger context (e.g., 20B tokens with 100k context)\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this converted into slides, diagrams, or tailored to a specific use case!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
